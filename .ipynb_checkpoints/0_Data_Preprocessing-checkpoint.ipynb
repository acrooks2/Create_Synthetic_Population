{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0:\n",
    "# Data Preproseccing\n",
    "* 1, Road Network Preprocessing\n",
    "* 2, Work Commute Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1, Road Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three steps to clean and get the giant connected component from the road shapefile.\n",
    "\n",
    "- Run GRASS `v.clean.advanced` tools `snap,break,rmdupl,rmsa` with tolerance values `0.0001,0.0,0.0,0.0`, save the result to `cleaned.shp`\n",
    "- Run GRASS `v.net.components` tool (`weak` or `strong` does not matter since the network is undirected), save the result as `giant_component.csv`\n",
    "- Using geoPandas combine the two files (shp and csv), filter the roads in the giant component, and save the result as `gcc.shp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pd.read_csv('../nWMDmap2/giant_component.csv', usecols=[0])\n",
    "cleaned = gpd.read_file('../nWMDmap2/cleaned.shp')\n",
    "\n",
    "roads = cleaned[['LINEARID','MTFCC','STATEFP','COUNTYFP','geometry']].join(components)\n",
    "roads = roads[roads.comp == 1610].drop('comp',axis=1)\n",
    "\n",
    "roads.to_file('../nWMDmap2/gcc.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2, Work Commute Data\n",
    "\n",
    "To get inter-tract commuting data at census-tract level:\n",
    "\n",
    "- Download the datasets (6*2 = 12 files in total)\n",
    "- Aggregate them at tract level (originial data is at block level, i.e. more granular)\n",
    "- Remove unincluded tracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD SCRIPT\n",
    "pre = 'https://lehd.ces.census.gov/data/lodes/LODES7/'\n",
    "#two separate files for the workers living in the same state and for those not\n",
    "for state in ['ny','nj','ct','pa','ri','ma']:\n",
    "    for res in ['main','aux']:\n",
    "        post = '_JT00_2010.csv.gz' if state != 'ma' else '_JT00_2011.csv.gz'\n",
    "        os.system('wget {0:}{1:}/od/{1:}_od_{2:}{3:}'.format(pre,state,res,post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in these columns only (ripping off the rest by `usecols=range(6)`):\n",
    "\n",
    "- S000: Total number of jobs\n",
    "- SA01: Number of jobs of workers age 29 or younger\n",
    "- SA02: Number of jobs for workers age 30 to 54\n",
    "- SA03: Number of jobs for workers age 55 or older"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRACT LEVEL O-D PAIRS\n",
    "#GEOID: state(2)-county(3)-tract(6): e.g. 09-001-030300\n",
    "census = gpd.read_file('../nWMDmap2/censusclip1.shp').set_index('GEOID10') #demographic profiles\n",
    "read_workflow = partial(pd.read_csv,usecols=range(6),dtype={0:str,1:str})\\\n",
    "\n",
    "wf = pd.concat([read_workflow(f) for f in glob('../od/*JT00*')]) #workflow\n",
    "wf['work'] = wf.w_geocode.str[:11]\n",
    "wf['home'] = wf.h_geocode.str[:11]\n",
    "\n",
    "od = wf[(wf.work.isin(census.index)) | (wf.home.isin(census.index))]\n",
    "od = od.groupby(['work','home']).sum()\n",
    "od.reset_index().to_csv('../od/tract-od.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
